{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e523624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<atp_analysis.ATPAnalyzer at 0x177430190>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from atp_analysis import ATPAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "base = \"july25_25/\"\n",
    "meta_path = \"/Volumes/Hadland_T7/Sequencing_Data/Fagradalsfjall/final_data/metadata_total.txt\"\n",
    "\n",
    "analyzer = (\n",
    "    ATPAnalyzer(data_csv=base + \"measurements_july25.csv\", time_csv=base + \"timestamps_all.csv\")\n",
    "      .integrate()\n",
    "      .fit_standard_curve()                               # 1) fit standards FIRST (raw integrals)\n",
    "      .fit_tris_drift(separate=True, split_seconds=1000)  # 2) Tris drift model (or separate=False)\n",
    "      .apply_corrections_and_quantify(\n",
    "            extract_vol=4.0,\n",
    "            sample_vol=pd.read_csv(base + \"atp_weights.csv\"),  # or a scalar like 5.0\n",
    "            sample_unit=\"g\"  # labels outputs as ng per g\n",
    "        )\n",
    "      .compute_blank_threshold(blank_names=['Blank 2','Blank 2.1','Blank 2.2'], k=3.0)  # optional\n",
    "      .merge_metadata(meta=meta_path, right_key=\"#SampleID\")  # optional\n",
    ")\n",
    "analyzer.plot_tris_drift(save_path=base+\"outputs/tris_drift.png\")\n",
    "analyzer.plot_standard_curve(save_path=base + \"outputs/ATP_Standard_Curve.png\")  # optional\n",
    "analyzer.plot_vs_metadata(x=\"Age\", y=\"avg_concentration\", save_path=base + \"outputs/atp_vs_age.png\")  # optional\n",
    "analyzer.save_outputs(prefix=base + \"outputs/atp_\")  # writes CSVs (integrals / samples_wide / grouped / merged_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d1930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: End-to-end ATP analysis with Tris-as-intercept workflow\n",
    "#\n",
    "# What you need on disk (wide-format):\n",
    "#   - data_csv:      a CSV with a \"Time\" column + one column per sample/standard/Tris/blank\n",
    "#                    (e.g., \"Time, 100, 100.1, Tris, Tris.1, SampleA, SampleA.1, Blank, ...\")\n",
    "#   - time_csv:      a 1-row CSV whose headers are the same sample names as columns in data_csv.\n",
    "#                    Each cell is a timestamp string (e.g., \"8/22/25 09:47\").\n",
    "#   - sample_vol CSV (optional): a CSV with columns ['Base_Sample','Sample_Volume'] to normalize\n",
    "#                    back to the original sample amount (e.g., grams soil, mL water).\n",
    "#   - metadata file (optional): CSV/TSV with per-sample info; must contain a key\n",
    "#                    column to join with Base_Sample (e.g., \"#SampleID\").\n",
    "#\n",
    "# Notes on naming conventions:\n",
    "#   • Standards are detected if the column name starts with a number (e.g., \"100\", \"10\", \"0.5\").\n",
    "#   • Tris controls are detected if \"Tris\" appears in the name (case-insensitive).\n",
    "#   • Replicates are grouped by stripping a trailing \".<n>\" (e.g., \"SampleA\", \"SampleA.1\", \"SampleA.2\" → Base_Sample = \"SampleA\").\n",
    "#   • Blanks: pass your actual blank names to compute_blank_threshold(blank_names=[...]). Optional\n",
    "#\n",
    "# Scientific model used here:\n",
    "#   • Fit the standard curve FIRST (slope & intercept) on raw integrals from standards.\n",
    "#   • Model Tris drift over time (one or two segments).\n",
    "#   • Correct each sample by subtracting the predicted Tris at that time (dynamic intercept).\n",
    "#   • Convert corrected luminescence to concentration using ONLY the slope:\n",
    "#         Conc_extract (ng/mL) = Corrected_Luminescence / slope\n",
    "#   • Optionally back-calc to ng per g (or per mL) of original sample using your Sample_Volume.\n",
    "\n",
    "from atp_analysis import ATPAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------\n",
    "# 1) Set up file locations\n",
    "# -----------------------\n",
    "# Use a base directory to keep outputs together\n",
    "base = \"example_run/\"  # change to your folder; it can be a nested path like \"results/2025-08-22/\"\n",
    "data_csv = base + \"measurements.csv\"       # your wide-format luminescence time series\n",
    "time_csv = base + \"timestamps.csv\"         # 1-row timestamps, headers must match columns in data_csv\n",
    "\n",
    "# Optional inputs (uncomment or replace with your own)\n",
    "sample_vol_csv = base + \"sample_volumes.csv\"   # must have columns: Base_Sample, Sample_Volume\n",
    "meta_path = base + \"metadata.tsv\"              # CSV/TSV; must have a join key (e.g., \"#SampleID\")\n",
    "\n",
    "# -----------------------\n",
    "# 2) Choose analysis knobs\n",
    "# -----------------------\n",
    "# Tris drift model:\n",
    "use_two_segment_tris = True         # set False to fit a single linear drift for Tris\n",
    "tris_split_seconds   = 1000         # only used if use_two_segment_tris=True; choose a sensible split for your run\n",
    "\n",
    "# Extraction / normalization parameters:\n",
    "extract_vol_mL = 4.0                # mL of Tris (or diluent) used in extraction\n",
    "# Three options for sample_vol in .apply_corrections_and_quantify():\n",
    "#   A) None  -> report ng/mL (extract) and Total_ng_in_extract\n",
    "#   B) float -> one scalar (e.g., grams soil or mL water) for ALL samples\n",
    "#   C) DataFrame -> per-sample volumes/weights via columns ['Base_Sample','Sample_Volume']\n",
    "#\n",
    "# Pick one:\n",
    "sample_vol_option = pd.read_csv(sample_vol_csv)     # Option C (most common): custom per-sample volumes\n",
    "# sample_vol_option = 5.0                           # Option B: every sample was 5 g (or 5 mL)\n",
    "# sample_vol_option = None                           # Option A: stay in extract units only\n",
    "sample_unit_label = \"g\"                             # used only when normalizing back to original sample; e.g., \"g\" or \"mL\"\n",
    "\n",
    "# Blank handling (optional):\n",
    "blank_names = ['Blank', 'Blank.1', 'Blank 2']       # set these to your actual blank column names\n",
    "k_sd = 3.0                                          # threshold = mean_blank + k * SD_blank\n",
    "\n",
    "# Metadata merge (optional):\n",
    "right_key_in_meta = \"#SampleID\"                     # column in your metadata that matches Base_Sample\n",
    "# If your metadata uses a different key (e.g., \"Sample\"), change right_key_in_meta accordingly.\n",
    "\n",
    "# Plot output paths (folders will be created automatically by the module):\n",
    "std_curve_png  = base + \"outputs/ATP_Standard_Curve.png\"\n",
    "meta_plot_png  = base + \"outputs/ATP_vs_Age.png\"\n",
    "\n",
    "# CSV output prefix (folder will be created automatically):\n",
    "csv_prefix     = base + \"outputs/atp_\"\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3) Run the pipeline (toggle options as needed)\n",
    "# ---------------------------------------------------\n",
    "analyzer = (\n",
    "    ATPAnalyzer(data_csv=data_csv, time_csv=time_csv)\n",
    "      .integrate()\n",
    "      .fit_standard_curve()                               # 1) Fit standards FIRST (raw integrals)\n",
    "      .fit_tris_drift(                                    # 2) Tris baseline drift model\n",
    "          separate=use_two_segment_tris,\n",
    "          split_seconds=tris_split_seconds if use_two_segment_tris else None\n",
    "      )\n",
    "      .apply_corrections_and_quantify(                    # 3) Subtract time-matched Tris; use slope only\n",
    "          extract_vol=extract_vol_mL,\n",
    "          sample_vol=sample_vol_option,                   # None | float | DataFrame(['Base_Sample','Sample_Volume'])\n",
    "          sample_unit=sample_unit_label                   # labels output as e.g. ng per g (only used if sample_vol is float or DataFrame)\n",
    "      )\n",
    "      .compute_blank_threshold(                           # 4) Optional: blank thresholding\n",
    "          blank_names=blank_names,\n",
    "          k=k_sd\n",
    "      )\n",
    "      .merge_metadata(                                    # 5) Optional: join with metadata\n",
    "          meta=meta_path,\n",
    "          right_key=right_key_in_meta\n",
    "      )\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4) Plots (folders auto-created)\n",
    "# ---------------------------------------------------\n",
    "# Standard curve figure:\n",
    "#   through_origin=False -> shows the original linear fit with intercept (for traceability)\n",
    "#   through_origin=True  -> draws a line through the origin (visual to match Tris-as-intercept math)\n",
    "analyzer.plot_standard_curve(save_path=std_curve_png, through_origin=True)\n",
    "\n",
    "# Plot average concentration vs a metadata column (requires merge_metadata earlier):\n",
    "#   Change x=... to any metadata column you want (e.g., \"Age\", \"Depth_cm\", \"pH\").\n",
    "analyzer.plot_vs_metadata(x=\"Age\", y=\"avg_concentration\", save_path=meta_plot_png)\n",
    "\n",
    "# Plot Tris drift and linear fits\n",
    "analyzer.plot_tris_drift(save_path=base+\"outputs/tris_drift.png\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5) CSV outputs (folder auto-created)\n",
    "# ---------------------------------------------------\n",
    "# Writes any tables that exist:\n",
    "#   - {prefix}integrals.csv         (integral & timestamp per column)\n",
    "#   - {prefix}samples_wide.csv      (aliquot-level concentrations and fields)\n",
    "#   - {prefix}grouped.csv           (mean±sd per Base_Sample)\n",
    "#   - {prefix}merged_meta.csv       (if metadata was merged)\n",
    "analyzer.save_outputs(prefix=csv_prefix)\n",
    "\n",
    "# -----------------------\n",
    "# Troubleshooting tips\n",
    "# -----------------------\n",
    "# • KeyError 'Corrected_Luminescence':\n",
    "#     Ensure you're running the latest module where apply_corrections_and_quantify()\n",
    "#     assigns the corrected DataFrame back to self.integrals_df BEFORE splitting.\n",
    "# • No standards detected:\n",
    "#     Make sure standard columns start with a number (e.g., \"100\", \"0.5\", not \"Std_100\").\n",
    "# • Timestamps parsing:\n",
    "#     time_csv must have a single row; headers match sample names in data_csv.\n",
    "#     The module understands formats like \"8/22/25 9:47\", \"2025-08-22 09:47:00\", \"HH:MM:SS\".\n",
    "# • Odd extra columns:\n",
    "#     The module auto-drops columns named \"Unnamed: xx\" and columns that are all-NaN (except \"Time\").\n",
    "# • Slope stability (optional diagnostic):\n",
    "#     If you suspect sensitivity drift (not just baseline), split standards early/late and compare\n",
    "#     through-origin slopes on corrected signals. If different, consider a piecewise slope."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
